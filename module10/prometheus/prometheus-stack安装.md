# prometheus-stack安装

## 前言

kube-prometheus-stack 包含Kubernetes 清单、Grafana 仪表板和 Prometheus 规则，借助 Prometheus Operator 通过 Prometheus 提供易于操作的端到端 Kubernetes 集群监控。

这个chart以前是`prometheus-operator`chart，现在改为了`kube-prometheus`项目栈，其中Prometheus Operator只是一个组件。



## 应用程序打包

### 应用程序

### Dockerfile

```bash
FROM golang:1.17 AS build
WORKDIR /httpserver/
COPY . .
ENV CGO_ENABLED=0
ENV GO111MODULE=on
ENV GOPROXY=https://goproxy.cn,direct
RUN GOOS=linux go build -installsuffix cgo -o httpserver main.go

FROM busybox
COPY --from=build /httpserver/httpserver /httpserver/httpserver
EXPOSE 8360
ENV ENV local
WORKDIR /httpserver/
ENTRYPOINT ["./httpserver"]
```

### 构建镜像命令

```bash
docker build -t httpserver-metric:v1 .
```

### 应用访问

```bash
➜  yaml git:(main) ✗ kubectl get svc -owide           
NAME         TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)          AGE     SELECTOR
httpsvc      NodePort    10.97.152.25     <none>        8080:31851/TCP   47s     app=httpserver
kubernetes   ClusterIP   10.96.0.1        <none>        443/TCP          7d2h    <none>
nginx-svc    ClusterIP   10.104.196.254   <none>        80/TCP           6d11h   app=nginx
You have new mail.                                                                                                     
➜  yaml git:(main) ✗ curl 10.97.152.25:8080
➜  yaml git:(main) ✗ curl 10.97.152.25:8080/healthz
working#                                                                                                               
➜  yaml git:(main) ✗ curl 10.97.152.25:8080/metrics
# HELP go_gc_cycles_automatic_gc_cycles_total Count of completed GC cycles generated by the Go runtime.
# TYPE go_gc_cycles_automatic_gc_cycles_total counter
go_gc_cycles_automatic_gc_cycles_total 0
# HELP go_gc_cycles_forced_gc_cycles_total Count of completed GC cycles forced by the application.
# TYPE go_gc_cycles_forced_gc_cycles_total counter
go_gc_cycles_forced_gc_cycles_total 0
# HELP go_gc_cycles_total_gc_cycles_total Count of all completed GC cycles.
# TYPE go_gc_cycles_total_gc_cycles_total counter
go_gc_cycles_total_gc_cycles_total 0
# HELP go_gc_duration_seconds A summary of the pause duration of garbage collection cycles.
# TYPE go_gc_duration_seconds summary
go_gc_duration_seconds{quantile="0"} 0
go_gc_duration_seconds{quantile="0.25"} 0
go_gc_duration_seconds{quantile="0.5"} 0
go_gc_duration_seconds{quantile="0.75"} 0
go_gc_duration_seconds{quantile="1"} 0
go_gc_duration_seconds_sum 0
go_gc_duration_seconds_count 0
# HELP go_gc_heap_allocs_by_size_bytes_total Distribution of heap allocations by approximate size. Note that this does not include tiny objects as defined by /gc/heap/tiny/allocs:objects, only tiny blocks.
# TYPE go_gc_heap_allocs_by_size_bytes_total histogram
go_gc_heap_allocs_by_size_bytes_total_bucket{le="8.999999999999998"} 5120
go_gc_heap_allocs_by_size_bytes_total_bucket{le="24.999999999999996"} 13650
go_gc_heap_allocs_by_size_bytes_total_bucket{le="64.99999999999999"} 19062
go_gc_heap_allocs_by_size_bytes_total_bucket{le="144.99999999999997"} 22744
go_gc_heap_allocs_by_size_bytes_total_bucket{le="320.99999999999994"} 24229
go_gc_heap_allocs_by_size_bytes_total_bucket{le="704.9999999999999"} 24790
go_gc_heap_allocs_by_size_bytes_total_bucket{le="1536.9999999999998"} 24964
go_gc_heap_allocs_by_size_bytes_total_bucket{le="3200.9999999999995"} 25052
go_gc_heap_allocs_by_size_bytes_total_bucket{le="6528.999999999999"} 25100
go_gc_heap_allocs_by_size_bytes_total_bucket{le="13568.999999999998"} 25128
go_gc_heap_allocs_by_size_bytes_total_bucket{le="27264.999999999996"} 25130
go_gc_heap_allocs_by_size_bytes_total_bucket{le="+Inf"} 25133
go_gc_heap_allocs_by_size_bytes_total_sum 2.489616e+06
go_gc_heap_allocs_by_size_bytes_total_count 25133
# HELP go_gc_heap_allocs_bytes_total Cumulative sum of memory allocated to the heap by the application.
# TYPE go_gc_heap_allocs_bytes_total counter
go_gc_heap_allocs_bytes_total 2.489616e+06
# HELP go_gc_heap_allocs_objects_total Cumulative count of heap allocations triggered by the application. Note that this does not include tiny objects as defined by /gc/heap/tiny/allocs:objects, only tiny blocks.
# TYPE go_gc_heap_allocs_objects_total counter
go_gc_heap_allocs_objects_total 25133
# HELP go_gc_heap_frees_by_size_bytes_total Distribution of freed heap allocations by approximate size. Note that this does not include tiny objects as defined by /gc/heap/tiny/allocs:objects, only tiny blocks.
# TYPE go_gc_heap_frees_by_size_bytes_total histogram
go_gc_heap_frees_by_size_bytes_total_bucket{le="8.999999999999998"} 0
go_gc_heap_frees_by_size_bytes_total_bucket{le="24.999999999999996"} 0
go_gc_heap_frees_by_size_bytes_total_bucket{le="64.99999999999999"} 0
go_gc_heap_frees_by_size_bytes_total_bucket{le="144.99999999999997"} 0
go_gc_heap_frees_by_size_bytes_total_bucket{le="320.99999999999994"} 0
go_gc_heap_frees_by_size_bytes_total_bucket{le="704.9999999999999"} 0
go_gc_heap_frees_by_size_bytes_total_bucket{le="1536.9999999999998"} 0
go_gc_heap_frees_by_size_bytes_total_bucket{le="3200.9999999999995"} 0
go_gc_heap_frees_by_size_bytes_total_bucket{le="6528.999999999999"} 0
go_gc_heap_frees_by_size_bytes_total_bucket{le="13568.999999999998"} 0
go_gc_heap_frees_by_size_bytes_total_bucket{le="27264.999999999996"} 0
go_gc_heap_frees_by_size_bytes_total_bucket{le="+Inf"} 0
go_gc_heap_frees_by_size_bytes_total_sum 0
go_gc_heap_frees_by_size_bytes_total_count 0
# HELP go_gc_heap_frees_bytes_total Cumulative sum of heap memory freed by the garbage collector.
# TYPE go_gc_heap_frees_bytes_total counter
go_gc_heap_frees_bytes_total 0
# HELP go_gc_heap_frees_objects_total Cumulative count of heap allocations whose storage was freed by the garbage collector. Note that this does not include tiny objects as defined by /gc/heap/tiny/allocs:objects, only tiny blocks.
# TYPE go_gc_heap_frees_objects_total counter
go_gc_heap_frees_objects_total 0
# HELP go_gc_heap_goal_bytes Heap size target for the end of the GC cycle.
# TYPE go_gc_heap_goal_bytes gauge
go_gc_heap_goal_bytes 4.473924e+06
# HELP go_gc_heap_objects_objects Number of objects, live or unswept, occupying heap memory.
# TYPE go_gc_heap_objects_objects gauge
go_gc_heap_objects_objects 25133
# HELP go_gc_heap_tiny_allocs_objects_total Count of small allocations that are packed together into blocks. These allocations are counted separately from other allocations because each individual allocation is not tracked by the runtime, only their block. Each block is already accounted for in allocs-by-size and frees-by-size.
# TYPE go_gc_heap_tiny_allocs_objects_total counter
go_gc_heap_tiny_allocs_objects_total 0
# HELP go_gc_pauses_seconds_total Distribution individual GC-related stop-the-world pause latencies.
# TYPE go_gc_pauses_seconds_total histogram
go_gc_pauses_seconds_total_bucket{le="-5e-324"} 0
go_gc_pauses_seconds_total_bucket{le="9.999999999999999e-10"} 0
go_gc_pauses_seconds_total_bucket{le="9.999999999999999e-09"} 0
go_gc_pauses_seconds_total_bucket{le="1.2799999999999998e-07"} 0
go_gc_pauses_seconds_total_bucket{le="1.2799999999999998e-06"} 0
go_gc_pauses_seconds_total_bucket{le="1.6383999999999998e-05"} 0
go_gc_pauses_seconds_total_bucket{le="0.00016383999999999998"} 0
go_gc_pauses_seconds_total_bucket{le="0.0020971519999999997"} 0
go_gc_pauses_seconds_total_bucket{le="0.020971519999999997"} 0
go_gc_pauses_seconds_total_bucket{le="0.26843545599999996"} 0
go_gc_pauses_seconds_total_bucket{le="+Inf"} 0
go_gc_pauses_seconds_total_sum NaN
go_gc_pauses_seconds_total_count 0
# HELP go_goroutines Number of goroutines that currently exist.
# TYPE go_goroutines gauge
go_goroutines 6
# HELP go_info Information about the Go environment.
# TYPE go_info gauge
go_info{version="go1.17.5"} 1
# HELP go_memory_classes_heap_free_bytes Memory that is completely free and eligible to be returned to the underlying system, but has not been. This metric is the runtime's estimate of free address space that is backed by physical memory.
# TYPE go_memory_classes_heap_free_bytes gauge
go_memory_classes_heap_free_bytes 0
# HELP go_memory_classes_heap_objects_bytes Memory occupied by live objects and dead objects that have not yet been marked free by the garbage collector.
# TYPE go_memory_classes_heap_objects_bytes gauge
go_memory_classes_heap_objects_bytes 2.489616e+06
# HELP go_memory_classes_heap_released_bytes Memory that is completely free and has been returned to the underlying system. This metric is the runtime's estimate of free address space that is still mapped into the process, but is not backed by physical memory.
# TYPE go_memory_classes_heap_released_bytes gauge
go_memory_classes_heap_released_bytes 5.382144e+06
# HELP go_memory_classes_heap_stacks_bytes Memory allocated from the heap that is reserved for stack space, whether or not it is currently in-use.
# TYPE go_memory_classes_heap_stacks_bytes gauge
go_memory_classes_heap_stacks_bytes 491520
# HELP go_memory_classes_heap_unused_bytes Memory that is reserved for heap objects but is not currently used to hold heap objects.
# TYPE go_memory_classes_heap_unused_bytes gauge
go_memory_classes_heap_unused_bytes 25328
# HELP go_memory_classes_metadata_mcache_free_bytes Memory that is reserved for runtime mcache structures, but not in-use.
# TYPE go_memory_classes_metadata_mcache_free_bytes gauge
go_memory_classes_metadata_mcache_free_bytes 9184
# HELP go_memory_classes_metadata_mcache_inuse_bytes Memory that is occupied by runtime mcache structures that are currently being used.
# TYPE go_memory_classes_metadata_mcache_inuse_bytes gauge
go_memory_classes_metadata_mcache_inuse_bytes 7200
# HELP go_memory_classes_metadata_mspan_free_bytes Memory that is reserved for runtime mspan structures, but not in-use.
# TYPE go_memory_classes_metadata_mspan_free_bytes gauge
go_memory_classes_metadata_mspan_free_bytes 11272
# HELP go_memory_classes_metadata_mspan_inuse_bytes Memory that is occupied by runtime mspan structures that are currently being used.
# TYPE go_memory_classes_metadata_mspan_inuse_bytes gauge
go_memory_classes_metadata_mspan_inuse_bytes 54264
# HELP go_memory_classes_metadata_other_bytes Memory that is reserved for or used to hold runtime metadata.
# TYPE go_memory_classes_metadata_other_bytes gauge
go_memory_classes_metadata_other_bytes 3.536552e+06
# HELP go_memory_classes_os_stacks_bytes Stack memory allocated by the underlying operating system.
# TYPE go_memory_classes_os_stacks_bytes gauge
go_memory_classes_os_stacks_bytes 0
# HELP go_memory_classes_other_bytes Memory used by execution trace buffers, structures for debugging the runtime, finalizer and profiler specials, and more.
# TYPE go_memory_classes_other_bytes gauge
go_memory_classes_other_bytes 1.442014e+06
# HELP go_memory_classes_profiling_buckets_bytes Memory that is used by the stack trace hash map used for profiling.
# TYPE go_memory_classes_profiling_buckets_bytes gauge
go_memory_classes_profiling_buckets_bytes 1.44525e+06
# HELP go_memory_classes_total_bytes All memory mapped by the Go runtime into the current process as read-write. Note that this does not include memory mapped by code called via cgo or via the syscall package. Sum of all metrics in /memory/classes.
# TYPE go_memory_classes_total_bytes gauge
go_memory_classes_total_bytes 1.4894344e+07
# HELP go_memstats_alloc_bytes Number of bytes allocated and still in use.
# TYPE go_memstats_alloc_bytes gauge
go_memstats_alloc_bytes 2.489616e+06
# HELP go_memstats_alloc_bytes_total Total number of bytes allocated, even if freed.
# TYPE go_memstats_alloc_bytes_total counter
go_memstats_alloc_bytes_total 2.489616e+06
# HELP go_memstats_buck_hash_sys_bytes Number of bytes used by the profiling bucket hash table.
# TYPE go_memstats_buck_hash_sys_bytes gauge
go_memstats_buck_hash_sys_bytes 1.44525e+06
# HELP go_memstats_frees_total Total number of frees.
# TYPE go_memstats_frees_total counter
go_memstats_frees_total 0
# HELP go_memstats_gc_cpu_fraction The fraction of this program's available CPU time used by the GC since the program started.
# TYPE go_memstats_gc_cpu_fraction gauge
go_memstats_gc_cpu_fraction 0
# HELP go_memstats_gc_sys_bytes Number of bytes used for garbage collection system metadata.
# TYPE go_memstats_gc_sys_bytes gauge
go_memstats_gc_sys_bytes 3.536552e+06
# HELP go_memstats_heap_alloc_bytes Number of heap bytes allocated and still in use.
# TYPE go_memstats_heap_alloc_bytes gauge
go_memstats_heap_alloc_bytes 2.489616e+06
# HELP go_memstats_heap_idle_bytes Number of heap bytes waiting to be used.
# TYPE go_memstats_heap_idle_bytes gauge
go_memstats_heap_idle_bytes 5.382144e+06
# HELP go_memstats_heap_inuse_bytes Number of heap bytes that are in use.
# TYPE go_memstats_heap_inuse_bytes gauge
go_memstats_heap_inuse_bytes 2.514944e+06
# HELP go_memstats_heap_objects Number of allocated objects.
# TYPE go_memstats_heap_objects gauge
go_memstats_heap_objects 25133
# HELP go_memstats_heap_released_bytes Number of heap bytes released to OS.
# TYPE go_memstats_heap_released_bytes gauge
go_memstats_heap_released_bytes 5.382144e+06
# HELP go_memstats_heap_sys_bytes Number of heap bytes obtained from system.
# TYPE go_memstats_heap_sys_bytes gauge
go_memstats_heap_sys_bytes 7.897088e+06
# HELP go_memstats_last_gc_time_seconds Number of seconds since 1970 of last garbage collection.
# TYPE go_memstats_last_gc_time_seconds gauge
go_memstats_last_gc_time_seconds 0
# HELP go_memstats_lookups_total Total number of pointer lookups.
# TYPE go_memstats_lookups_total counter
go_memstats_lookups_total 0
# HELP go_memstats_mallocs_total Total number of mallocs.
# TYPE go_memstats_mallocs_total counter
go_memstats_mallocs_total 25133
# HELP go_memstats_mcache_inuse_bytes Number of bytes in use by mcache structures.
# TYPE go_memstats_mcache_inuse_bytes gauge
go_memstats_mcache_inuse_bytes 7200
# HELP go_memstats_mcache_sys_bytes Number of bytes used for mcache structures obtained from system.
# TYPE go_memstats_mcache_sys_bytes gauge
go_memstats_mcache_sys_bytes 16384
# HELP go_memstats_mspan_inuse_bytes Number of bytes in use by mspan structures.
# TYPE go_memstats_mspan_inuse_bytes gauge
go_memstats_mspan_inuse_bytes 54264
# HELP go_memstats_mspan_sys_bytes Number of bytes used for mspan structures obtained from system.
# TYPE go_memstats_mspan_sys_bytes gauge
go_memstats_mspan_sys_bytes 65536
# HELP go_memstats_next_gc_bytes Number of heap bytes when next garbage collection will take place.
# TYPE go_memstats_next_gc_bytes gauge
go_memstats_next_gc_bytes 4.473924e+06
# HELP go_memstats_other_sys_bytes Number of bytes used for other system allocations.
# TYPE go_memstats_other_sys_bytes gauge
go_memstats_other_sys_bytes 1.442014e+06
# HELP go_memstats_stack_inuse_bytes Number of bytes in use by the stack allocator.
# TYPE go_memstats_stack_inuse_bytes gauge
go_memstats_stack_inuse_bytes 491520
# HELP go_memstats_stack_sys_bytes Number of bytes obtained from system for stack allocator.
# TYPE go_memstats_stack_sys_bytes gauge
go_memstats_stack_sys_bytes 491520
# HELP go_memstats_sys_bytes Number of bytes obtained from system.
# TYPE go_memstats_sys_bytes gauge
go_memstats_sys_bytes 1.4894344e+07
# HELP go_sched_goroutines_goroutines Count of live goroutines.
# TYPE go_sched_goroutines_goroutines gauge
go_sched_goroutines_goroutines 6
# HELP go_sched_latencies_seconds Distribution of the time goroutines have spent in the scheduler in a runnable state before actually running.
# TYPE go_sched_latencies_seconds histogram
go_sched_latencies_seconds_bucket{le="-5e-324"} 0
go_sched_latencies_seconds_bucket{le="9.999999999999999e-10"} 102
go_sched_latencies_seconds_bucket{le="9.999999999999999e-09"} 102
go_sched_latencies_seconds_bucket{le="1.2799999999999998e-07"} 102
go_sched_latencies_seconds_bucket{le="1.2799999999999998e-06"} 113
go_sched_latencies_seconds_bucket{le="1.6383999999999998e-05"} 125
go_sched_latencies_seconds_bucket{le="0.00016383999999999998"} 158
go_sched_latencies_seconds_bucket{le="0.0020971519999999997"} 159
go_sched_latencies_seconds_bucket{le="0.020971519999999997"} 159
go_sched_latencies_seconds_bucket{le="0.26843545599999996"} 159
go_sched_latencies_seconds_bucket{le="+Inf"} 159
go_sched_latencies_seconds_sum NaN
go_sched_latencies_seconds_count 159
# HELP go_threads Number of OS threads created.
# TYPE go_threads gauge
go_threads 6
# HELP process_cpu_seconds_total Total user and system CPU time spent in seconds.
# TYPE process_cpu_seconds_total counter
process_cpu_seconds_total 0.02
# HELP process_max_fds Maximum number of open file descriptors.
# TYPE process_max_fds gauge
process_max_fds 65536
# HELP process_open_fds Number of open file descriptors.
# TYPE process_open_fds gauge
process_open_fds 9
# HELP process_resident_memory_bytes Resident memory size in bytes.
# TYPE process_resident_memory_bytes gauge
process_resident_memory_bytes 1.5863808e+07
# HELP process_start_time_seconds Start time of the process since unix epoch in seconds.
# TYPE process_start_time_seconds gauge
process_start_time_seconds 1.65624736386e+09
# HELP process_virtual_memory_bytes Virtual memory size in bytes.
# TYPE process_virtual_memory_bytes gauge
process_virtual_memory_bytes 7.29411584e+08
# HELP process_virtual_memory_max_bytes Maximum amount of virtual memory available in bytes.
# TYPE process_virtual_memory_max_bytes gauge
process_virtual_memory_max_bytes 1.8446744073709552e+19
# HELP promhttp_metric_handler_requests_in_flight Current number of scrapes being served.
# TYPE promhttp_metric_handler_requests_in_flight gauge
promhttp_metric_handler_requests_in_flight 1
# HELP promhttp_metric_handler_requests_total Total number of scrapes by HTTP status code.
# TYPE promhttp_metric_handler_requests_total counter
promhttp_metric_handler_requests_total{code="200"} 0
promhttp_metric_handler_requests_total{code="500"} 0
promhttp_metric_handler_requests_total{code="503"} 0
➜  yaml git:(main) ✗ 

```

## 安装

https://github.com/prometheus-community/helm-charts/tree/main/charts/kube-prometheus-stack

```go
helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
helm repo update

helm -n prometheus-stack install kube-prometheus-stack prometheus-community/kube-prometheus-stack
```

### 镜像问题

```bash
Failed to pull image "k8s.gcr.io/ingress-nginx/kube-webhook-certgen:v1.1.1": rpc error: code = Unknown desc = Error response from daemon: Get https://k8s.gcr.io/v2/: net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)
```

> k8s.gcr.io镜像下载的代理方式
> 把k8s.gcr.io改成lank8s.cn就可以轻松使用k8s.gcr.io的镜像了
> 或者
> https://github.com/Fly0905/gcr.io_mirror

```bash
docker pull lank8s.cn/ingress-nginx/kube-webhook-certgen:v1.1.1
docker tag lank8s.cn/ingress-nginx/kube-webhook-certgen:v1.1.1 k8s.gcr.io/ingress-nginx/kube-webhook-certgen:v1.1.1

docker pull lank8s.cn/kube-state-metrics/kube-state-metrics:v2.5.0
docker tag lank8s.cn/kube-state-metrics/kube-state-metrics:v2.5.0 registry.k8s.io/kube-state-metrics/kube-state-metrics:v2.5.0
```

## 配置

```bash
➜  kubectl get prometheuses -n prometheus-stack
NAME                               VERSION   REPLICAS   AGE
kube-prometheus-stack-prometheus   v2.36.1   1          21m
```

如果改动的配置不生效，查看opeator log

```go
➜  kubectl get pod -n prometheus-stack kube-prometheus-stack-operator-8f9b4d79b-7ttwf
NAME                                             READY   STATUS    RESTARTS   AGE
kube-prometheus-stack-operator-8f9b4d79b-7ttwf   1/1     Running   0          33m
```

## 自动发现

我们在每个节点上面都运行了 node-exporter，如果我们通过一个 Service 来将数据收集到一起用静态配置的方式配置到 Prometheus 去中，就只会显示一条数据，我们得自己在指标数据中去过滤每个节点的数据，当然我们也可以手动的把所有节点用静态的方式配置到 Prometheus 中去，但是以后要新增或者去掉节点的时候就还得手动去配置，那么有没有一种方式可以让 Prometheus 去自动发现我们节点的 node-exporter 程序，并且按节点进行分组呢？这就是 Prometheus 里面非常重要的**「服务发现」**功能。

Prometheus支持多种服务发现机制：**文件、DNS、Consul、Kubernetes、OpenStack、EC2等**。基于服务发现的过程并不复杂，通过第三方提供的接口，Prometheus查询到需要监控的Target列表，然后轮训这些Target获取监控数据，下面主要介绍Kubernetes服务发现机制。

### 创建发现规则

我们定义的 Prometheus 的配置如下 `prometheus-additional.yaml`

- endpoints role 从每个服务监听的 endpoints 发现，每个 endpoint 都会发现一个port，

- 如果 endpoint 是一个pod，所有包含的容器不被绑定到一个 endpoint port，也会被 targets 被发现。

- 如果你对上面这个配置还不是很熟悉的话，建议去查看下前面关于 Kubernetes 常用资源对象监控的介绍，要想自动发现集群中的 Service，**就需要我们在 Service 的 annotation 区域添加**  `**prometheus.io/scrape=true**`  **的声明**

```yaml
cat > prometheus-additional.yaml <<"EOF"
- job_name: 'kubernetes-endpoints'
  kubernetes_sd_configs:
    - role: endpoints
  relabel_configs:
    - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scrape]
      action: keep
      regex: true
    - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scheme]
      action: replace
      target_label: __scheme__
      regex: (https?)
    - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_path]
      action: replace
      target_label: __metrics_path__
      regex: (.+)
    - source_labels: [__address__, __meta_kubernetes_service_annotation_prometheus_io_port]
      action: replace
      target_label: __address__
      regex: ([^:]+)(?::\d+)?;(\d+)
      replacement: $1:$2
    - action: labelmap
      regex: __meta_kubernetes_service_label_(.+)
    - source_labels: [__meta_kubernetes_namespace]
      action: replace
      target_label: kubernetes_namespace
    - source_labels: [__meta_kubernetes_service_name]
      action: replace
      target_label: kubernetes_name
    - source_labels: [__meta_kubernetes_pod_name]
      action: replace
      target_label: kubernetes_pod_name   
EOF
```

存入集群的secret

```bash
# 创建secret
kubectl create secret generic additional-configs --from-file=prometheus-additional.yaml -n  prometheus-stack
# 查看secret
kubectl get secret additional-configs -n  prometheus-stack -oyaml
```



### 注入Prometheus

然后我们需要在声明 prometheus 的资源对象文件中通过 additionalScrapeConfigs 属性添加上这个额外的配置：

```bash
kubectl edit prometheuses -n prometheus-stack kube-prometheus-stack-prometheus
```

```go
root@dev:~# kubectl get prometheuses -n prometheus-stack -oyaml
apiVersion: v1
items:
- apiVersion: monitoring.coreos.com/v1
  kind: Prometheus
  metadata:
    annotations:
      meta.helm.sh/release-name: kube-prometheus-stack
      meta.helm.sh/release-namespace: prometheus-stack
    creationTimestamp: "2022-03-11T05:47:59Z"
    generation: 2
    labels:
      app: kube-prometheus-stack-prometheus
      app.kubernetes.io/instance: kube-prometheus-stack
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/part-of: kube-prometheus-stack
      app.kubernetes.io/version: 33.2.0
      chart: kube-prometheus-stack-33.2.0
      heritage: Helm
      release: kube-prometheus-stack
    name: kube-prometheus-stack-prometheus
    namespace: prometheus-stack
    resourceVersion: "4059202"
    uid: d819465f-f360-4124-8eaa-97997ada65a5
  spec:
    # 通过 additionalScrapeConfigs 属性添加上这个额外的配置
    additionalScrapeConfigs:
      key: prometheus-additional.yaml
      name: additional-configs
    alerting:
      alertmanagers:
      - apiVersion: v2
        name: kube-prometheus-stack-alertmanager
        namespace: prometheus-stack
        pathPrefix: /
        port: http-web
    enableAdminAPI: false
    externalUrl: http://kube-prometheus-stack-prometheus.prometheus-stack:9090
    image: quay.io/prometheus/prometheus:v2.33.4
    listenLocal: false
    logFormat: logfmt
    logLevel: info
    paused: false
    podMonitorNamespaceSelector: {}
    podMonitorSelector:
      matchLabels:
        release: kube-prometheus-stack
    portName: http-web
    probeNamespaceSelector: {}
    probeSelector:
      matchLabels:
        release: kube-prometheus-stack
    replicas: 1
    retention: 10d
    routePrefix: /
    ruleNamespaceSelector: {}
    ruleSelector:
      matchLabels:
        release: kube-prometheus-stack
    securityContext:
      fsGroup: 2000
      runAsGroup: 2000
      runAsNonRoot: true
      runAsUser: 1000
    serviceAccountName: kube-prometheus-stack-prometheus
    serviceMonitorNamespaceSelector: {}
    serviceMonitorSelector:
      matchLabels:
        release: kube-prometheus-stack
    shards: 1
    storage:
      volumeClaimTemplate:
        spec:
          resources:
            requests:
              storage: 5Gi
          storageClassName: default
    version: v2.33.4
kind: List
metadata:
  resourceVersion: ""
  selfLink: ""
```

```bash
level=warn ts=2022-06-26T00:38:39.990742109Z caller=operator.go:328 component=prometheusoperator msg="failed to check if the API supports the endpointslice resources" err="converting (v1.APIGroup) to (v1.APIResourceList): unknown conversion"
```

重启下prometheus pod.

```bash
kubectl rollout restart deployment -n prometheus-stack kube-prometheus-stack-operator

kubectl get deployment -n prometheus-stack kube-prometheus-stack-operator


kubectl get pod -n prometheus-stack -l app=kube-prometheus-stack-operator -owide

-n prometheus-stack kube-prometheus-stack-operator
```



#### k8s | 重启Kubernetes Pod的几种方式

> - ## 前言
>
>   在使用 docker 的过程中，我们可以使用`docker restart {container_id}`来重启容器，但是在 kubernetes 中并没有重启命令（没有 `kubectl restart {podname}`），有时候我们的 Pod 出现 Bug意外终止，导致我们需要重启 Pod ，却没有一个很好的方式，特别是没有 yaml 文件的情况下，所以本文总结了以下几种重启 Pod 的方式。
>
>   ## 方法 1
>
>   有最新的 yaml 文件。
>
>   在有 yaml 文件的情况下可以直接使用`kubectl replace --force -f xxxx.yaml`来强制替换Pod 的 API 对象，从而达到重启的目的。
>
>   ## 方法 2
>
>   没有 yaml 文件，但是使用的是 Deployment 对象。
>
>   ```shell
>   kubectl scale deployment esb-admin --replicas=0 -n {namespace}
>   kubectl scale deployment esb-admin --replicas=1 -n {namespace}
>   ```
>
>   由于 Deployment 对象并不是直接操控的 Pod 对象，而是操控的 ReplicaSet 对象，而 ReplicaSet 对象就是由副本的数目的定义和Pod 模板组成的。所以这条命令分别是将ReplicaSet 的数量 scale 到 0，然后又 scale 到 1，那么 Pod 也就重启了。
>
>   ## 方法 3
>
>   同样没有 yaml 文件，但是使用的是 Deployment 对象。
>
>   使用命令`kubectl delete pod {podname} -n {namespace}`
>
>   这个方法就很简单粗暴了，直接把 Pod 删除，因为 Kubernetes 是声明式 API，所以删掉了之后，Pod API 对象就与预期的不一致了，所以会自动重新创建 Pod 保持与预期一致，但是如果ReplicaSet 管理的 Pod 对象很多的话，那么要一个个手动删除，会很麻烦，所以可以使用`kubectl delete replicaset {rs_name} -n {namespace}`命令来删除 ReplicaSet
>
>   ## 方法 4
>
>   没有 yaml 文件，直接使用的 Pod 对象。
>
>   使用命令`kubectl get pod {podname} -n {namespace} -o yaml | kubectl replace --force -f -`
>
>   在这种情况下，由于没有 yaml 文件，且启动的是 Pod 对象，那么是无法直接删除或者 scale 到 0 的，但可以通过上面这条命令重启。这条命令的意思是 get 当前运行的 pod 的 yaml声明，并管道重定向输出到 kubectl replace命令的标准输入，从而达到重启的目的。
>
>   ## 总结
>
>   我们可以通过多种方式来重启对象，总的来说，最推荐的方式是使用`kubectl get pod {podname} -n {namespace} -o yaml | kubectl replace --force -f -`这种方式，因为适用于多种对象。此外，重启 Pod 并不会修复运行程序的 bug，想要解决程序的意外终止，最终还是得要修复 bug。



### 配置RBAC

```yaml
cat > clusterrole-prometheus-k8s.yaml <<"EOF"
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: prometheus-k8s
rules:
  - apiGroups:
      - ""
    resources:
      - nodes
      - services
      - endpoints
      - pods
      - nodes/proxy
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - ""
    resources:
      - configmaps
      - nodes/metrics
    verbs:
      - get
  - nonResourceURLs:
      - /metrics
    verbs:
      - get
EOF
```

### 在svc上打上annoation

```bash
kubectl edit svc -n prometheus-stack kube-prometheus-stack-prometheus


```



```go
kind: Service
metadata:
  annotations:
    # 在svc上打上annoation
    prometheus.io/port: "8080"
    prometheus.io/scrape: "true"
  creationTimestamp: "2022-03-12T03:59:18Z"
  labels:
    app: httpserver
  name: httpsvc
  namespace: cloudnative
  resourceVersion: "4037833"
  uid: 98091bb2-33f4-4cc7-a48c-dfcf0ddcdfe7
spec:
  clusterIP: 10.0.67.117
  clusterIPs:
  - 10.0.67.117
  ipFamilies:
  - IPv4
  ipFamilyPolicy: SingleStack
  ports:
  - port: 80
    protocol: TCP
    targetPort: 8080
  selector:
    app: httpserver
  sessionAffinity: None
  type: ClusterIP
status:
  loadBalancer: {}
```

### 查看Prometheus原生监控图

```go
kubectl port-forward -n prometheus-stack  --address 0.0.0.0 svc/kube-prometheus-stack-prometheus 9090:9090
```

#### 检查 Status下的  Configration 和 Service Discovery, Targets

#### 服务发现

```bash
http://10.0.41.95:9090/service-discovery?search=
```

![image-20220626212222236](../AppData/Roaming/Typora/typora-user-images/image-20220626212222236.png)

[Prometheus Time Series Collection and Processing Server](http://192.168.31.212:9090/classic/service-discovery)

#### Target

```bash

kubectl get ep
NAME         ENDPOINTS                                 AGE
httpsvc      192.168.219.79:8080,192.168.219.80:8080   36m

```



![image-20220626212313146](../AppData/Roaming/Typora/typora-user-images/image-20220626212313146.png)

[Prometheus Time Series Collection and Processing Server](http://192.168.31.212:9090/classic/targets)

![image-20220626213633404](../AppData/Roaming/Typora/typora-user-images/image-20220626213633404.png)



#### PromQL基本语法

- 查询

```plain
histogram_quantile(0.50, sum(rate(cloudnative_execution_latency_seconds_bucket[5m])) by (le))
```

> Prometheus Query Language
> histogram_quantile(0.95,sum(rate(httpserver_execution_latency_seconds_bucket[5m]))by (le))
> ·Histogram是直方图，httpserver_execution_latency_seconds bucke是直方图指标，是将
> httpserver处理请求的时间放入不同的桶内，其表达的是落在不同时长区间的响应次数。
> ·by(le),是将采集的数据按桶的上边界分组。
> 。rate(httpserver_.execution._latency_seconds_bucket[5m]),计算的是五分钟内的变化率。
> ·Sum(),是将所有指标的变化率总计。
> ·0.95,是取95分位。
> 综上：上述表达式计算的是httpserver处理请求时，95%的请求在五分钟内，在不同响应时间区间的处
> 理的数量的变化情况。

![image-20220627072441680](../AppData/Roaming/Typora/typora-user-images/image-20220627072441680.png)

#### PromQL手册

https://prometheus.io/docs/prometheus/latest/querying/basics/

https://yunlzheng.gitbook.io/prometheus-book/parti-prometheus-ji-chu/promql

#### 监控的四个黄金指标

Four Golden Signals是Google针对大量分布式监控的经验总结，4个黄金指标可以在服务级别帮助衡量终端用户体验、服务中断、业务影响等层面的问题。主要关注与以下四种类型的指标：延迟，通讯量，错误以及饱和度：



-  **延迟：服务请求所需时间。**
  记录用户所有请求所需的时间，重点是要区分成功请求的延迟时间和失败请求的延迟时间。 例如在数据库或者其他关键祸端服务异常触发HTTP 500的情况下，用户也可能会很快得到请求失败的响应内容，如果不加区分计算这些请求的延迟，可能导致计算结果与实际结果产生巨大的差异。除此以外，在微服务中通常提倡“快速失败”，开发人员需要特别注意这些延迟较大的错误，因为这些缓慢的错误会明显影响系统的性能，因此追踪这些错误的延迟也是非常重要的。 

-  **通讯量（流量/吞吐量）**：监控当前系统的流量，用于衡量服务的容量需求。
  流量对于不同类型的系统而言可能代表不同的含义。例如，在HTTP REST API中, 流量通常是每秒HTTP请求数； 

-  **错误：监控当前系统所有发生的错误请求，衡量当前系统错误发生的速率。**
  对于失败而言有些是显式的(比如, HTTP 500错误)，而有些是隐式(比如，HTTP响应200，但实际业务流程依然是失败的)。 



对于一些显式的错误如HTTP 500可以通过在负载均衡器(如Nginx)上进行捕获，而对于一些系统内部的异 

常，	则可能需要直接从服务中添加钩子统计并进行获取。



- **饱和度（容量）：衡量当前服务的饱和度。**
  主要强调最能影响服务状态的受限制的资源。 例如，如果系统主要受内存影响，那就主要关注系统的内存状态，如果系统主要受限与磁盘I/O，那就主要观测磁盘I/O的状态。因为通常情况下，当这些资源达到饱和后，服务的性能会明显下降。同时还可以利用饱和度对系统做出预测，比如，“磁盘是否可能在4个小时候就满了”。



## grafana



port forward 到本地



```go
kubectl port-forward -n prometheus-stack  --address 0.0.0.0 svc/kube-prometheus-stack-grafana 9000:80
```



注意 promstack里的grafana是弱密码，所以千万不要在没有改密码的情况下，将svc暴露到公网！







导入json







```go
{
  "annotations": {
    "list": [
      {
        "builtIn": 1,
        "datasource": "-- Grafana --",
        "enable": true,
        "hide": true,
        "iconColor": "rgba(0, 211, 255, 1)",
        "name": "Annotations & Alerts",
        "target": {
          "limit": 100,
          "matchAny": false,
          "tags": [],
          "type": "dashboard"
        },
        "type": "dashboard"
      }
    ]
  },
  "editable": true,
  "fiscalYearStartMonth": 0,
  "graphTooltip": 0,
  "id": 34,
  "links": [],
  "liveNow": false,
  "panels": [
    {
      "datasource": {
        "type": "prometheus",
        "uid": "prometheus"
      },
      "fieldConfig": {
        "defaults": {
          "color": {
            "mode": "palette-classic"
          },
          "custom": {
            "axisLabel": "",
            "axisPlacement": "auto",
            "barAlignment": 0,
            "drawStyle": "line",
            "fillOpacity": 0,
            "gradientMode": "none",
            "hideFrom": {
              "legend": false,
              "tooltip": false,
              "viz": false
            },
            "lineInterpolation": "linear",
            "lineWidth": 1,
            "pointSize": 5,
            "scaleDistribution": {
              "type": "linear"
            },
            "showPoints": "auto",
            "spanNulls": false,
            "stacking": {
              "group": "A",
              "mode": "none"
            },
            "thresholdsStyle": {
              "mode": "off"
            }
          },
          "mappings": [],
          "thresholds": {
            "mode": "absolute",
            "steps": [
              {
                "color": "green",
                "value": null
              },
              {
                "color": "red",
                "value": 80
              }
            ]
          }
        },
        "overrides": [
          {
            "__systemRef": "hideSeriesFrom",
            "matcher": {
              "id": "byNames",
              "options": {
                "mode": "exclude",
                "names": [
                  "histogram_quantile(0.50, sum(rate(cloudnative_execution_latency_seconds_bucket[5m])) by (le))"
                ],
                "prefix": "All except:",
                "readOnly": true
              }
            },
            "properties": [
              {
                "id": "custom.hideFrom",
                "value": {
                  "legend": false,
                  "tooltip": false,
                  "viz": true
                }
              }
            ]
          }
        ]
      },
      "gridPos": {
        "h": 9,
        "w": 12,
        "x": 0,
        "y": 0
      },
      "id": 2,
      "options": {
        "legend": {
          "calcs": [],
          "displayMode": "list",
          "placement": "bottom"
        },
        "tooltip": {
          "mode": "single"
        }
      },
      "targets": [
        {
          "exemplar": true,
          "expr": "histogram_quantile(0.95, sum(rate(cloudnative_execution_latency_seconds_bucket[5m])) by (le))",
          "interval": "",
          "legendFormat": "",
          "refId": "A"
        },
        {
          "exemplar": true,
          "expr": "histogram_quantile(0.90, sum(rate(cloudnative_execution_latency_seconds_bucket[5m])) by (le))",
          "hide": false,
          "interval": "",
          "legendFormat": "",
          "refId": "B"
        },
        {
          "exemplar": true,
          "expr": "histogram_quantile(0.50, sum(rate(cloudnative_execution_latency_seconds_bucket[5m])) by (le))",
          "hide": false,
          "interval": "",
          "legendFormat": "",
          "refId": "C"
        }
      ],
      "title": "Panel Title",
      "type": "timeseries"
    }
  ],
  "refresh": "",
  "schemaVersion": 34,
  "style": "dark",
  "tags": [],
  "templating": {
    "list": []
  },
  "time": {
    "from": "now-1m",
    "to": "now"
  },
  "timepicker": {},
  "timezone": "",
  "title": "Http Server Latency",
  "uid": "mWgwgx5nz",
  "version": 2,
  "weekStart": ""
}
```



# 参考链接

https://github.com/prometheus-community/helm-charts/tree/main/charts/kube-prometheus-stack

https://artifacthub.io/packages/helm/prometheus-community/kube-prometheus-stack/

把k8s.gcr.io改成lank8s.cn就可以轻松使用k8s.gcr.io的镜像了

